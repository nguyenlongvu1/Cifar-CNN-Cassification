# CIFAR-10 Image Classification with CNN (PyTorch)

This project trains a Convolutional Neural Network (CNN) on the CIFAR-10 dataset using PyTorch. It includes features like training-validation splitting, loss visualization, learning rate scheduling, and test evaluation with a confusion matrix.

## ğŸ“ Project Structure
```
â”œâ”€â”€ train.py # Training script
â”œâ”€â”€ evaluate.py # Evaluate model on test set
â”œâ”€â”€ model.py # CNN model definition
â”œâ”€â”€ visualize.py # Functions for plotting loss and confusion matrix
â”œâ”€â”€ cnn_model.pth # Saved trained model
â”œâ”€â”€ README.md # This file
â””â”€â”€ data/ # CIFAR-10 dataset will be downloaded here
```

## ğŸ§  Model Architecture
```
Conv2D(3 â†’ 32) + ReLU + BatchNorm
Conv2D(32 â†’ 64) + ReLU + BatchNorm + MaxPool
Conv2D(64 â†’ 128) + ReLU + BatchNorm
Conv2D(128 â†’ 128) + ReLU + BatchNorm + MaxPool
AdaptiveAvgPool2d â†’ Flatten
Dropout â†’ Linear(128 â†’ 128) â†’ ReLU
Dropout â†’ Linear(128 â†’ 10)
```

## ğŸ“Š Visualizations

- Training and validation loss per epoch
  ![Training Loss](./loss.png)
- Confusion matrix on test set
  ![Training Loss](./confusion_matrix.png)
## âœ… Example Results

- **Test Accuracy (Custom CNN)**: ~79.95%  

---

## ğŸ”§ Model Improvement Suggestions (for Custom CNN)

To improve the performance of the custom CNN, consider the following enhancements:

- âœ… **Data Augmentation**: Add transforms such as `RandomCrop`, `RandomHorizontalFlip`, or `ColorJitter` to improve generalization.
- âœ… **Increase Depth or Width**: Add more convolutional layers or increase the number of filters.
- âœ… **Use Residual Connections**: Incorporate skip connections inspired by ResNet to help training deeper networks.
- âœ… **Replace Fully Connected Layers with Global Pooling**: Use `GlobalAvgPool2d` + `Linear(128 â†’ 10)` for more efficient representation.
- âœ… **Label Smoothing**: A regularization technique that can help reduce overconfidence of the model.
- âœ… **Advanced Optimizers**: Try optimizers like `SGD with momentum`, `RAdam`, or `AdamW`.
- âœ… **Learning Rate Scheduling**: Already used `ReduceLROnPlateau`, but other schedulers like `CosineAnnealingLR` may improve convergence.

---

## ğŸ—ï¸ Transfer Learning with ResNet-18 (Pretrained)

To further improve performance with fewer epochs, we also experimented with a pretrained **ResNet-18** model from `torchvision.models`.

### Modifications:
- Resized input images to 224Ã—224.
- Replaced the final FC layer with `nn.Linear(512, 10)` for CIFAR-10.
- Frozen all convolutional layers and fine-tuned only the fully connected layer.

### Results:
- **Test Accuracy**: ~82.55% after only 6 epochs of training.
- Demonstrates the power of transfer learning for small datasets.

---



